services:
  # Required by Kafka (non-KRaft mode) to manage broker metadata,
  # leader election, and cluster coordination.
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      # Port used by Kafka to connect to ZooKeeper
      ZOOKEEPER_CLIENT_PORT: 2181
      # Basic timing configuration for ZooKeeper internals
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      # Expose ZooKeeper port to the host
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    # Ensure ZooKeeper container starts before Kafka
    depends_on:
      - zookeeper
    ports:
      # External Kafka port exposed to the host machine
      - "29092:29092"
    environment:
      # Unique broker ID (single-broker setup)
      KAFKA_BROKER_ID: 1
      # ZooKeeper connection string
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"

      # - INTERNAL: for communication inside Docker network
      # - EXTERNAL: for communication from host machine
      KAFKA_LISTENERS: "INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:29092"

      # Addresses advertised to Kafka clients
      # INTERNAL is used by Docker services, EXTERNAL by host clients
      KAFKA_ADVERTISED_LISTENERS: "INTERNAL://kafka:9092,EXTERNAL://localhost:29092"

      # Map listener names to security protocols
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT"

      # Listener used for inter-broker communication
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"

      # Replication settings adjusted for single-broker development setup
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1

      # Automatically create topics if they do not exist
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

  # Reads data from MySQL and publishes it as JSON events to Kafka
  producer:
    build:
      context: ./services/producer
    container_name: producer
    environment:
      # Kafka bootstrap server inside Docker network
      KAFKA_BOOTSTRAP: kafka:9092
      KAFKA_TOPIC: shots_raw
      MYSQL_HOST: host.docker.internal
      MYSQL_PORT: 8889
      MYSQL_USER: root
      MYSQL_PASSWORD: root
      MYSQL_DATABASE: basketball_optimalization
      PYTHONUNBUFFERED: "1"

    depends_on:
      kafka:
        condition: service_started
    restart: "no"
  
  # Consumes raw events from Kafka and writes them to S3
  # as newline-delimited JSON in a bronze data layer
  bronze_s3_writer:
    build:
      context: ./services/bronze_s3_writer
    container_name: bronze_s3_writer
    environment:
      PYTHONUNBUFFERED: "1"
      KAFKA_BOOTSTRAP: kafka:9092
      KAFKA_TOPIC: shots_raw
      KAFKA_GROUP_ID: bronze-s3-writer
      AWS_REGION: ${AWS_REGION}
      S3_BUCKET: ${S3_BUCKET}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      S3_PREFIX: bronze/shots_raw
      # If writer also touches MySQL, keep these:
      MYSQL_HOST: mysql
      MYSQL_PORT: 3306
      MYSQL_USER: pipeline
      MYSQL_PASSWORD: pipeline
      MYSQL_DATABASE: basketball_optimalization
    depends_on:
      kafka:
        condition: service_started
      mysql:
        condition: service_healthy
    restart: "no"

  airflow-postgres:
    image: postgres:15
    user: "0:0"
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_pgdata:/var/lib/postgresql/data
  mysql:
    image: mysql:8.0
    container_name: mysql
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: basketball_optimalization
      MYSQL_USER: pipeline
      MYSQL_PASSWORD: pipeline
    volumes:
      - mysql_data:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-proot"]
      interval: 5s
      timeout: 3s
      retries: 30

  airflow:
    build:
      context: ./airflow
    depends_on:
      airflow-postgres:
        condition: service_started
      mysql:
        condition: service_healthy
      kafka:
        condition: service_started
    env_file:
      - ./airflow/.env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
      DATABRICKS_CLUSTER_ID: 0203-015823-an9cos24
      AWS_REGION: ${AWS_REGION}
      S3_BUCKET: ${S3_BUCKET}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./:/opt/airflow/project
    ports:
      - "8080:8080"
    command: >
      bash -lc "
      airflow db init &&
      airflow users create
        --username admin
        --password admin
        --firstname Admin
        --lastname Admin
        --role Admin
        --email admin@example.com || true;
      airflow scheduler & exec airflow webserver
      "

volumes:
  airflow_pgdata:
  mysql_data:
